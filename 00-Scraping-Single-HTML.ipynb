{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/icon_important.jpg\" width=\"50\" align=\"left\"/>\n",
    "</div>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### __Important Legal Notice__\n",
    "By running and editing this Jupyter notebook with the corresponding dataset, you agree that you will not use or store the data for other purposes than participating in the Champagne Coding with DNB & Women in Data Science, Oslo. You will delete the data and notebook after the event and will not attempt to identify any of the commentors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will review two methods for scraping reviews:\n",
    "- __Method 1__: Download HTML directly from the website\n",
    "- __Method 2__: Using a crawler using a web driver and the application ID\n",
    "\n",
    "Both of these methods require some element of personalization. When using __Method 1__, you will first need to scroll to the end of the reviews in order to get access to the HTML of all the reviews. You will then need to use the \"Inspect\" tool in your web browser to find the unique identifiers for the elements of the review (name, date, review score, review text, etc). We've identified these elements for the DNB application, but they may be different for other applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "current_directory = Path.cwd()\n",
    "reviews_directory = Path(current_directory, 'reviews')\n",
    "html_directory = Path(current_directory, 'html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Use HTML downloaded directly from Google Play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this method to work, you need to first have a saved html file in the ```html``` directory. You can download the HTML for the application by first finding the application you are interested in through the Google Play store. \n",
    "\n",
    "In this case, we will go through the steps for downloading the HTML for the DNB app (https://play.google.com/store/apps/details?id=no.apps.dnbnor&hl=en&showAllReviews=true)\n",
    "\n",
    "We start by navigating to the google play store and finding the application we are interested in. This will show you a page similar to this:\n",
    "<br>\n",
    "<br>\n",
    "<div>\n",
    "<img src=\"images/00-google_play_store_screenshot.png\" width=\"500\"/>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "Scroll all the way to the bottom of the reviews, clicking to show all reviews. This may take some time depending on the popularity of the application and number of reviews.\n",
    "\n",
    "We will download the HTML using the Inspect option since we need to understand the elements of the code for our scraping purposes. In Google Chrome, you can access this by right clicking somewhere on the page and clicking ```Inspect```\n",
    "<br>\n",
    "<br>\n",
    "<div>\n",
    "<img src=\"images/01-inspect.png\" width=\"500\"/>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "To save the HTML, you can either copy the highest level element, or find the element that includes the comments. \n",
    "<br>\n",
    "<br>\n",
    "<div>\n",
    "<img src=\"images/01-copy_html.png\" width=\"500\"/>\n",
    "</div>\n",
    "<br>\n",
    "Save the copied element in a text editor. I've saved mine as ```dnb.html``` and placed it in a directory called ```html```, located within this folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that you've saved your HTML, we're ready to extract some information.\n",
    "\n",
    "We will use BeautifulSoup, a Python library that makes it easy to scrape information from web pages. Using an HTML parser, it provides useful methods for iterating, searching, and modifying the parse tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by reading our saved HTML file. Here, we will read our html file ```dnb.html``` which is located in our html directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file = open(Path(html_directory, 'dnb.html'), 'r', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where our inspection job becomes important. When using the inspect tool in Google Chrome, we found the names and key-value pairs for each element we are interested in. \n",
    "\n",
    "__Note__: These will be different for different applications so make sure that you update these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what we've found for the DNB application:\n",
    "- __Entire Review & Contents__: ```div jscontroller=\"H6eOGe\"```\n",
    "- __Name__: ```span class=\"X43Kjb\"```\n",
    "- __Date__: ```span class=\"p2TkOb\"```\n",
    "- __Review Score__: ```div class=\"pf5lIe\"```\n",
    "- __Review Text__: ```span jsname=\"fbQN7e\"```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by converting our html string that we've read in to a BeautifulSoup object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(read_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find all of the comments, so let's go back to our inspect tool and look at what the review elements are called. Here we can see that they are div element, with the following attribute name pair: ```'jscontroller': 'H6eOGe'```\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<div>\n",
    "<img src=\"images/all_comments.png\" width=\"500\"/>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "We will use that knowledge to search for all comments with these attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments = soup.body.find_all('div', attrs={'jscontroller': 'H6eOGe'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to find the corresponding elements for name, date, score, and the review text. Adjust the parameters within accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loop through each of the comments and use a beautiful soup function to find the relevant parts\n",
    "all_reviews_dict={}\n",
    "i = 0\n",
    "\n",
    "for each_comment in all_comments:\n",
    "    \n",
    "    current_review = {}\n",
    "\n",
    "    name = each_comment.find('span', attrs= {'class': 'X43Kjb'})\n",
    "    current_review['Name'] = name.text \n",
    "\n",
    "    date = each_comment.find('span', attrs= {'class': 'p2TkOb'})\n",
    "    current_review['Date'] = date.text \n",
    "\n",
    "    score = each_comment.find('div', attrs= {'class': 'pf5lIe'})\n",
    "    current_review['Review_Score'] = re.search('(\\d+) stars out of five stars', str(score)).group(1)\n",
    "\n",
    "    review_text = each_comment.find('span', attrs= {'jsname': 'bN97Pc'}) #jsname=\"bN97Pc\"\n",
    "    current_review['Review_Text'] = review_text.text\n",
    "    i += 1\n",
    "    \n",
    "    all_reviews_dict[i] = current_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this look as a data frame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allreviews = pd.DataFrame(all_reviews_dict)\n",
    "df_allreviews = df_allreviews.T\n",
    "\n",
    "df_allreviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many reviews do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_allreviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for duplicates and empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allreviews.drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's delete completely duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allreviews.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any empty cells?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allreviews.dropna().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anonymization\n",
    "\n",
    "Since there were names in some of the reviews, we decided to use the list of names from the scraped reviews and replace them with empty strings in the review text. Make sure that you always prioritize anonymization when working with potentially sensitive data :) \n",
    "\n",
    "If you added new app reviews that contain the names, make sure to run this function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "def remove_names(input_string, list_names):\n",
    "    for n in list_names:\n",
    "        input_string = input_string.replace(n.lower(), \"\")\n",
    "        \n",
    "    return input_string\n",
    "\n",
    "def anonymization(df):\n",
    "    \"\"\"\n",
    "    Create a list of names and search for them within the review text. \n",
    "    Replaces these names with blank strings.\n",
    "    \"\"\"\n",
    "    # Store all of the names to a list\n",
    "    all_names = df.Name.tolist()\n",
    "    # Take the set of the list of names that are longer than 5\n",
    "    names_list = list(set([n for n in all_names if len(n) > 5]))\n",
    "    \n",
    "    df.Review_Text = df.Review_Text.apply(lambda x: remove_names(str(x).lower(), names_list))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allreviews = anonymization(df_allreviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allreviews[['Date', \n",
    "               'Review_Score', \n",
    "               'Review_Text']].to_csv(Path(reviews_directory, 'dnb_reviews.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Use a crawler and the app id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use selenium's webdriver API that allows us to control a browser running locally. There exists many drivers for the different web browsers. Here we will use Chrome. If you don't have chrome installed, you can choose an Internet Explorer, Firefox, or Safari web driver. \n",
    "\n",
    "More information [here](https://www.seleniumhq.org/projects/webdriver/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "import requests\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First find the name of the application you would like to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_id = 'no.apps.dnbnor&hl=en'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the driver\n",
    "\n",
    "This will open a Chrome instance and search for the application ID you've set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "link = \"https://play.google.com/store/apps/details?id={}\".format(app_id)\n",
    "driver.get(link + '&showAllReviews=true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start crawling the webpage.\n",
    "\n",
    "In this loop, there are two options - click if there is an element called ```Show More``` or we will scroll continuously if the element isn't present. The loop will only stop once the max number of clicks or the max number of scrolls has been reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__ : This can take some time depending on how many ```max_clicks``` you set.\n",
    "\n",
    "##### __TODO__:  Setting the number of maximum clicks is arbitrary in this case. How could you set up this method so that it scrolls and clicks just enough rather than continuing until the max criteria is reached?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this number to get more or less reviews\n",
    "max_clicks = 40\n",
    "\n",
    "# Start crawling\n",
    "num_clicks = 0\n",
    "num_scrolls = 0\n",
    "while num_clicks <= max_clicks and num_scrolls <= max_clicks*5:\n",
    "    try:\n",
    "        show_more = driver.find_elements_by_xpath(\"//*[contains(text(), 'Show More')]\")\n",
    "        show_more[0].click()\n",
    "        \n",
    "        num_clicks += 1\n",
    "    except:\n",
    "        html = driver.find_element_by_tag_name('html')\n",
    "        html.send_keys(Keys.END)\n",
    "        num_scrolls +=1\n",
    "        time.sleep(1)\n",
    "\n",
    "print('Done scrolling')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we're ready to evaluate the HTML we've scraped.\n",
    "\n",
    "We'll use ```BeautifulSoup``` again to parse the html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(driver.page_source, 'html.parser')\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will find the major H2 categories of the page. You can see that one of them is called ```Reviews```. This is what we are interested in parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2 = soup.find_all('h2')\n",
    "h2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's get parsing! This will use the same code as above\n",
    "\n",
    "##### __TODO__ : Feeling brave and ambitious? How do you think this could be done in a way that wouldn't require inspecting the HTML for the name as we did earlier? Does there seem to be a method to the madness?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments = soup.body.find_all('div', attrs={'jscontroller': 'H6eOGe'})\n",
    "\n",
    "### Loop through each of the comments and use a beautiful soup function to find the relevant parts\n",
    "all_reviews_dict={}\n",
    "i = 0\n",
    "\n",
    "for each_comment in all_comments:    \n",
    "    current_review = {}\n",
    "\n",
    "    name = each_comment.find('span', attrs= {'class': 'X43Kjb'})\n",
    "    current_review['Name'] = name.text \n",
    "\n",
    "    date = each_comment.find('span', attrs= {'class': 'p2TkOb'})\n",
    "    current_review['Date'] = date.text \n",
    "\n",
    "    score = each_comment.find('div', attrs= {'class': 'pf5lIe'})\n",
    "    current_review['Review_Score'] = re.search('(\\d+) stars out of five stars', str(score)).group(1)\n",
    "\n",
    "    review_text = each_comment.find('span', attrs= {'jsname': 'bN97Pc'}) #jsname=\"bN97Pc\"\n",
    "    current_review['Review_Text'] = review_text.text\n",
    "    i += 1\n",
    "\n",
    "    all_reviews_dict[i] = current_review\n",
    "\n",
    "df_allreviews_driver = pd.DataFrame(all_reviews_dict)\n",
    "df_allreviews_driver = df_allreviews_driver.T\n",
    "\n",
    "df_allreviews_driver.drop_duplicates(inplace=True)\n",
    "print(\"Done reading {} application data. {} reviews were found.\".format('DNB',\n",
    "                                                                        len(df_allreviews_driver)))\n",
    "\n",
    "# Anonymize the reviews using the anonymization function\n",
    "df_allreviews_driver = anonymization(df_allreviews_driver)\n",
    "\n",
    "df_allreviews_driver[['Date', \n",
    "               'Review_Score', \n",
    "               'Review_Text']].to_csv(Path(reviews_directory, '{}_reviews_from-webdriver.csv'.format('DNB')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many reviews were extracted with this method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allreviews_driver.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it the same length as the original file where we saved the HTML ourselves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allreviews.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anonymize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allreviews_driver = anonymization(df_allreviews_driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allreviews_driver = df_allreviews_driver.reset_index(drop=True)\n",
    "df_allreviews_driver[['Date', \n",
    "            'Review_Score',\n",
    "            'Review_Text']].to_csv(Path(reviews_directory, 'dnb_reviews-autoparsed.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to start analyzing! Continue to the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
